
Okay. So now we'd like to write thoughts about code-generation for SQL. What are we trying to do? Well, we're trying to to make it easy to define tables, as a first step. That means writing something like 

```
device :: Table
device = table "devices" do
  field "name" do
    string
    
  field "nickname" do
    string *> nil
    
  field "address" do
    string *> primary
    
exercise :: Table
exercise = table "exercises" do
  field "id" do
    int *> primary

  field "name" do
    string
  
  field "device" do
    string *> foreign "devices" "address"
    
human :: Table
human = table "humans" do
  field "id" do int
  
  field "name" do string
    
  field "height" do int 
    
  field "parent" do int *> nil
    
  primaryKey [ "id", "name"]

  foreignKey [ "parent" ]
```

The above defines a bunch of tables, using a Table DSL and a Field DSL. It all builds up to a Table data structure, which has a list of fields and primary/foreign key definitions. Defining the tables, and their relations to each other, is a _minimum_ step in defining a database -- and we do it all in the runtime, so that we can analyze it. We can now add each table to a builder, and pass it into a verification phase -- to verify field attributes, to verify that foreign keys exist, and to verify that all attributes are in valid combinations, and not over-specified, and to verify that multiple fields aren't specified, and to _force_ the specification of a single primary key for every table, so that primary keys always exist.

Based on the table definitions, we can then turn each table into a create statement, either for pure SQL as a (parameterized?) query, or into a full set of purescript statements, defining a DeviceTable and a function for creating the table if it doesn't exist. All of it would proceed from the table definition, and we could then use it to define the table in the generated script.

```
create :: Table -> PsName -> PsString
create table name = implementation

write file $ create device "DeviceTable"
```

Thereafter, then, we would have a basis for submitting simple records to the database. Which is all well, and good, but frequently we want to do more. We want to autogenerate complex representations of the data that can _still_ be submitted to the database to be inserted or saved. So how can we do so, given that most of our fields are going to be non-null? Well, we can allow specification of default values at the field level:

```
field "name" do
  string *> default "NO NAME"
```

And while that may have no concept within the database itself, it does have a role when it comes to the definitions of _everything_ else, because when we now define more complex data based on the table, the default value will be available. So for example, if a field has no default value yet is _omitted_ in some way from the complex type, then the database compilation process will err -- no default value for a non-null field means no insertion process can be defined for the complex type.
  But if we have default values, it means we can now _omit_ fields from our complex data types. Great; that reduces the burden. So what kind of complex data can we define? Well, we can define simple remaps of fields:

```
simpleHuman :: ActiveRecord
simpleHuman = activeRecord "SimpleHuman" device do
  "id" := "id"
  "fullName" := "name"
  "pounds" := "weight"
  
  default "height" 7
```

This lets us produce what is essentially an "active record" piece of data -- a data item thats us expose a subset of the fields for a data item, to make it easier to work with in application code, because we are only worked with _some_ fields. We can also change the database defaults for a given type, like "height" in the above example. If the default value is the wrong type for the field, then the compilation process will catch this. 
  Note that primary keys MUST appear on the ActiveRecord, since we need enough data to save the ActiveRecord back to the table, for updates, in most cases.
  But who don't typically only want a few useless fields. We also often want _relational_ fields in some way. A Human might want to see parent and grandparent, if they exist. How do we ensure that? This is a join, but it's a join to produce, recursively, a SimpleHuman that contains a SimpleHuman, in terms of type. We might
write 

```
simpleHuman :: ActiveRecord
simpleHuman = activeRecord "SimpleHuman" device do
  "id" := "id"
  ...
  "parent" := one "SimpleHuman" >> from "parentId"
  "children" := many "SimpleHuman" >> from "parentId" >> match "id"
```

The `one ... from` and `many ... from` descriptors allow us to specify simple relationsips that are based on simple joins between two tables, where the tables are inferred from the backing table for the ActiveRecord, and the joined item is obtained from foreign keys, which we can compile to _verify_ are actually foreign keys. By default, the foreign key will go to the primary key, but we can use `match` to override the field used, if we want a separate "unique" field to be used instead.

However, we often want more-complex kinds of queries and data, like what I said before about grandparents, or even grandchildren. How would we handle a double-query, and multiple-join, that is needed to get that data? In the case above, we can walk the parent and child tree, but suppose we don't want to. More generally, how do we conceive of data that relies on joins? Well, in that case, SQL joins are the best way to express this. But for this to be efficient, instead of querying once per in the database, what would we have to do? We would have to describe how the foreign item was obtained, possibly for multiple foreign tables --- but this time, we can check parse and check the query on our own.

```
grandParent :: String -> Query 
grandParent id = 

simpleHuman :: ActiveRecord
simpleHuman = activeRecord "SimpleHuman" "humans" do
  "id" := "id"
  ...
  "grandparent" := subquery "select gp.id, gp.name from ..." >> asOne "SimpleHuman"
  "grandchildren" := subquery "select gc.id, gc.name from ..." >> asMany "SimpleHuman"
```

It thus becomes clear that the ActiveRecord may be exposing _multiple_ relationships at once to the simple human. We may be executing the queries immediately -- and with one-from and many-from, it may make out, since we might be able to perform one or more inner joins to get the right result (maybe?). But if we're not, we may necessarily have to syncronously perform the query when the collection is accessed -- performing the query once for each human that is accessed. There may be no other choice.

But we have to be careful anyway -- we can, in theory, pull _all_ data from the simple human query, as many queries have to be performed to get the children. Likewise, since parents is open-ended, we might end up pulling all _parents_ as well. In that sense, recursive data is dangerous -- and thus, relationships cannot truly be exposed data that we edit from the relationship itself.

And that's good. It answers an important question about relationships, because the _update_ of relationships as data is tricky, because we then have to merge it with the current items in the database, which is no easy feat, since the merge may be complex -- the total items being sent back may have extra members compared to the old items, or fewer items -- what does that mean? It's hard to say. So if relationships _cannot_ be data, but are instead some kind of cursor that pulls out the typed data on request, then there is no issue, and we are capable of doing a few different things. First, we can iterate throughout all the data, performing an action on each one. Second, we can do so _repeatedly_ when need to go through the data again. This isn't needed for any "one" relationship, but it's useful for any "many" relationship. And for special kinds of relationships, where the relationship is direct in a well-defined way, using `one...from` or `many...from`, or using a defined many-to-many table, we can add to the relationship directly using something like `relate rel new`. Which will add to the relationship, and _now_, whenever the relationship query is used for iteration again, it will fetch the newly added value. This goes for both one and many relations. This is much more difficult for the special-queries that don't have special metadata -- but that's unavoidable, and just a necessary consequence. Those queries are ReadonlyRel objects, so they can be used to read, but something more complex is needed to add to them, because the complex query relationship makes things more difficult.
  Notice as well the `asOne SimpleHuman` in the example above. We specify a query, but we can (and must) specify if we are trying to make it one, or many, so that the raw database data is hidden behind a type. This will be determined by analysis of the query -- the fields in the query _must_ come from the right table and match the table. 
  
Alternatively, we can allow "select gc.*" so that we specify the table, and allow the type to select the fields for the ActiveRecord type.

Also, in such a complex relationship -- how are the relationships updated? Because it's conceivable that that data we pull out will be old -- if it goes back in without a version number, it will overwrite old relationship data -- either by deleting new data, or inserting old data; either way, the relationship is not an accurate way of representing the data -- it is merely a snapshot. And so what really happens is that the relationship data must _track_ what has been added or removed from the original data, to track what changes are made to the relationship data. But that becomes excessively complex.

Now, the above is fine if all we want to do is quickly define ActiveRecord objects. It's easy, in the sense that we can quickly get simple and reasonable APIs, while type-checking the production of these APIs against each other. This may be enough for most cases. But what happens when we want more-complex representations than that? The ActiveRecord API is fine for modifying an individual record, but it's very difficult to turn it into other, more-flattened data types. What do we do? 
  Well, I suspect that for most cases, it's better to do nothing. There's no reason to try to force a strange DSL for a general data-mapped object type -- although we could try, it's hard to see how that would work, unless it's just concatenaning ActiveRecords together to act as one record. Which is possible, but not useful. Instead, what we need is the power of queries more generally, to obtain data from the database in the desired form, _even if_ we can't reasonably modify and update it through any means. We need generalized readonly queries for the purpose of having data sources that are well-typed despite being written in SQL. Thus, we would write 

```
allSports :: Query
allSports = query """
  SELECT s.name, s.phone, s.minAge l.name FROM
    sports s JOIN league l on s.league = l.id
    WHERE
    s.player_count > this.minCount
  """
```

What we obtain is a formally described query. We did not include the names of the tables, since we will join with those tables later. We also have the 'this' concept, to represent an input object with values -- by comparing the min value to specific database fields, we end up able to infer its type from what we compared it to; in sophisticated future cases, we might even widen the type for things like numerical comparisons.
  Anyway, this query is largely executed as-is, with the exception of being parameterized, and of becoming a function that might do particular type conversions. In so doing, we can take the query and execute it at runtime to obtain an iterable cursor for us to go over the results. We can't edit them, but we can extract data on this basis and use it to build ActiveRecord objects with results -- in particular, we rely on the generation of ActiveRecord constructors that enable us to create an empty ActiveRecord from a base of data, for later insertion.
  That's probably the best we can do for queries -- we cannot guarantee modification of the data in any way for arbitrary queries, but we can guarantee type-checked queries and generation of functions that produce Queries that can be instantiated and run.

So we have tables, ActiveRecords, and Queries. What it means is that writing the database code can all be done in a limited DSL, that minimizes typing. The question, then, is this: How does it all get put together? How would we expect to compile it and write it to file? 
  Well, unlike when we parse and compile a file, we don't have any compiler state that is obtained by walking an AST. Instead, we have most of our compile state externally. So to compile, we need to build the key compiler state, which is the tables and ActiveRecords and queries. We need to know _everything_ that exists.

```
do
  compiler <- newCompiler 
    { tables: [...]
    , records: [...]
    , queries: [...]
    }
```
   
The compiler is loaded with this data. It thus have the ability to validate _everything_ at this point. Calling `validate compiler` can step through the tables to validate them, then through the records and the queries as well, following references where necessary to look up other records, or other queries, by name, and typechecking everything, publishing errors where needed. Any SQL queries can be parsed and compiled and checked as well, at this stage.
  Thus, the compiler can perform all the static checks. It then needs, or wants, to _write_ queries. Now, we can do this in one of two ways. First, we can do it manually. We can submit each individual table to the compiler, and each individual query, and use the File API to choose a destination directory and destination file. We can have the compiler AssertProjectRoot, and call `compiler.done` at the end to verify that all the included tables, records, and queries were submitted for compilation to a string, even if we don't know to-where they were written.
  But as is usually the case, this is a pain in the ass. We can automate this. We can specify an output directory and a base-prefix module, and then start default generation from there. For simplicity's sake, we can place all initial types in a "Class" file, so that it can be shared everywhere. We can define all tables in a "Tables" file, since their types are also used everywhere, and define a single "createTables" function that will set up all tables for the database. ActiveRecords may also reference each other, producing each other in relationships, so they _also_ have to be defined in one file, "Records". And finally, queries will likely reference tables, and produce output data types of their own, so they get their own files as well, and receive their own separate names for each query.
  So rather surprisingly, perhaps, it's easiest to group initial group definitions according to how types reference each other.
  Once all initial definitions are done, _exports_ can be generated so that particular types are grouped together. So for example, a "Device" file might be generated with basic operations on the Device table itself (besides creating everything). Alternatively, although we might _define_ all types in single files like Queries, Records, and Tables, all the _implementations_ do get separated -- so all the subsequent DeviceRecord functionality goes in a "DeviceRecord" file. Yes, that seems correct.
  Likewise, we might expect Queries to belong to a particular type, since they don't belong to tables, so that multiple queries for the same Type can be grouped together. For example, we might define 

```
sportAndLeague :: QueryType
sportAndLeague = queryType "SportAndLeague" do
  field "name" do string
  field "age" do int *> nil
  
allSportsAndLeague :: Query
allSportsAndLeague = query "SportAndLeague" 
  """
  select ...
  """
```

The query would thus be type-checked against the type, instead of generating an anonymous type for every individual query. "SportAndLeague" would therefore become its own separate file, for all queries related to it.
  In this way, we can define all the aspects of the database, and nevertheless generate according to the resulting types that are produced, while performing type-checking and schema-checking.
  
Finally, migrations. Migrations are a bit bizarre here. To enable them, we try to expose our code to _only_ the ActiveRecords and Query types, so that a migration can occur without affecting the public API of the database. However, perhaps this isn't always possible. Under what conditions is it possible? Under what conditions is it not?
  It's possible if we _add_ a field. But if we add a not-null field, we have a problem -- it won't have a value, so how can we add the field? SQL generaly provides an "Alter Table" statement that can be used to rename, add, or delete columns, or to modify a column's datatype. Or we can rename the table entirely. But it's never easy, and not all schema changes are actually simple. So can we auto-migrate, or do we need specific steps to be executed for a given table?
  It's hard to say that we can auto-migrate. Even for default values, it isn't necessarily simple to do so -- adding a new non-null column will REQUIRE a default value for the database. And it's possible that a column might need a _calculated_ value based on the existing row -- that isn't necessarily simple, either, but makes a default value possibly _invalid_ entirely, unless we use a default value at first, and then turn it off afterward.
  In other words ... it's necessarily messy. An auto-migration might not actually be possible, in which case we should be able to easily specify a sequence of manual steps to perform the migration ... but if we do so, how can we be sure that things still typecheck, in the end? How can we be sure that our proposed migration steps actually accomplish what we want? 

Well, there's a few things.

- First, there's the runtime schema. It is certainly possible for us to 
  verify that _after_ the schema changes, the final, new schema is correct. It's 
  also possible (necessary?) to verify that the _initial_ schema is the 
  expected schema.
- Second, schema changes can occur as part of a transaction, always, to avoid   
  interruptions that ruin corrupt the database.
- Third, at compile-time, we have the initial schema and the target schema. 
  So we _know_ what changes need to be made. In one form or another, we should be 
  able to analyze the proposed actions to verify that they are sequenced correctly, 
  and that all the desired schema changes are carried out. Which means that migrations 
  need a clear set of descriptors for schema changes that are just _data_, so we can 
  analyze them.
  
But just this isn't enough for migrations. We need to be able to specify _versions_. And more than that, we need to be able to able to specify and store the _list_ of version changes, so that no matter how many schema changes we make, and no matter the starting schema on the device, when the database update finally happens, it follows the same path as _all_ other devices, so we don't see varying behavior based on multiple different paths. We have one consistent path through the versions. But how do we store the versions?
  Operationally, the version for generating queries _must_ be the latest one. And we can _specify_ that version, too. But the migration path code is almost a _separate_ runtime than the operational code that we're generating for the database. But for a migration, because multiple tables might be involved in one version change, and certain tables might have to be migrated in particular orders ... what do we do? Do we need to store _all_ the tables? Or just the changed tables? What should occur? What actually needs to be stored?
  App versions like this really need to be stored in _separate_ files -- that's how large they are. Something like 

```
schema1 :: Schema
schema1 = version 1 
  [ devices
  , tuners
  , humans ...
  ]
```

The schema will list the version and all the tables, which are defined in the same file
as the schema. Then, in a separate Implementation file, we choose a schema version, and _through_ it, we use all the tables. In a separate Migrations file, we import all the schema versions, and use them to write a "migrations" script. A migration is required for _each_ schema version, and all schema from 1 to N are required to be part of the build process, and only tables from _1_ schema are allowed to be part of the Implementation. In each new schema version, the prior number must be imported, and we describe a migration as follows:

```
schema2 :: Schema
schema2 = version 2
  [ devices
  , ...
  ]
  $ from (schema 1) migrate
  
migrate :: Migration
migrate = migration do
  dropTable "devices"
  alterColumn "tuner.name" ...
  addColumns "scanner" do
    field "maxTries" do int *> tempDefault 3

  updateEach "scanner" $ trimMargin """
  | setDefaults :: Scanner -> Scanner
  | setDefaults s = s { maxTries = s.minTries + 5 }
  
  defineQuery "allAges" "select abc ..."

  updateEachWithDb "scanner" $ trimMargin """
  | setDefaults :: Db -> Scanner -> Aff Scanner
  | setDefaults db = do
      query db allAges \s -> do
        ...
      pure newScanner
  """
```

As you can see above, generating the migration file may involve defining arbitrary functions using the generated runtime types that _don't_ exist yet. But that's just the way it is -- there's no other way it could be; the second compilation phase will catch any errors we make in defining the migration function.
  Of course, this is hardly ideal. It's very, very much not ideal, but for each scanner, we can still write queries to grab values at runtime (potentially) for updating each value, as needed. It's ... difficult to do certain things this way, though, since we are missing particular types. We _might_ allow definition of models here for the express purpose of pre- and post- operations for migrations.
  The problem with trying to do it operationally is that the latest version numbers will be _wrong_, and that each migration is trying to define operations that will be useful for changing the _data_. There are schema changes and there are data changes, but they aren't the same, even if they're both necessary at roughly the same time. In that case, what does it _mean_ to want to change the data? And how do we want to change it? As we walk version numbers, the database environment changes and we ourselves want to make changes. So for the purposes of data population, we do need _temporary_ environments that match the needs of the database. These environments will only truly exist at runtime in certain points in the migration. But how do we say that?
  In the most naive case, we say that by just saying "We execute SQL statements". That's fine up to a point, but it leaves such statements completely untyped by the targeted schema. Yet on the other hand, the schema changes we make may be temporary. We might have a temporary column to execute a schema change, and then delete that column. If columns are being added and deleted in this way, then we can hardly be sure of what the data type is. Are schema changes truly so arbitrary? Do we need to migrate schema through _minor_ version changes as well, as part of the migration? Are all schema changes even truly possible?
  For data changes to work, and to be safe, the schema must be established and not in a bizarre state. Thus, if a migration requires multiple steps and data edits, multiple minor versions are needed to reach the major version, and each such version needs to be documented. Each such version thus obtains the _right_ to create models and queries, and to use those models and queries to update the database. What's interesting is that as a result, each model ends up type-checked against the database -- but an _old_ version of the database. And thus, perhaps we should think of the Database as something that _publishes_ objects, such that each object's operations _always_ perform a version check at runtime to verify the version is correct as it runs. Thus, we don't just have versioned schema -- each database is _also_ versioned, to provide and retain full power for each version. We can also _drop_ versions as time goes on -- for particularly old versions, we can even consider publishing an 'error' message for being too out-of-date, if we need to, to require reinstallation of the app for versions that are old (by date, for example). These database versions all exist side-by-side,  but all refer to a different version of the database. And that makes sense: when doing migrations, all versions of the database DO exist at runtime, at particular points. So it's understandable that we would do this, and then _only_ publish the latest version for public use in one set of export modules.
  But assuming we do indeed have this, as we say, what is the process for launching the migrations and initializing the database, and running any needed migrations before allowing the application to touch anything? We would expect:

1. Check the database for the VERSION table. If it doesn't exist, create it, and create the tables for the target version. Since there was no version, we are safe to create the latest version without doing any migrations.
2. If the VERSION table exists, check the version. If it's an unknown version, err. If it's a known version, get the metadata about the version and validate it against the runtime metadata; if there are discrepancies, err. 
3. If everything matches, determine the migration path along versions, from the current version to the target version of the application, with any minor versions included along the way.
4. Once we have a migration path, execute the migration path associated with each transition. This will require starting a tx, changing the schema, running any data updates, and so on. Once all the paths have finished, we can close the transaction; if an error occurs, we rollback the transaction.

The question, then, is how we determine the individual migration paths. The migration paths rely on the differences in version schema to verify that the requested operations are _desired_, and allowed, and not violating any constraints (like existing columns or non-existent columns). We try to verify as much as we can at compile-time analysis of the action, since we know we're going to auto-generate it anyway. And this is fine for the proposed column actions. But it's not for any data generation that needs to occur to make the column changes make sense. Thus, a version must specify data actions that need to occur after the version change occurs. Frequently, simple SQL will not be expressive enough to do this; instead, we rely on a Record model to do so -- that can do so because it is aware that a column is new and needs to be populated from other records/columns.
  But there's a problem -- the Model that needs to execute the data changes does _not_ exist in the space of the table definition. So how do we include it, or the queries, that would be needed for the data changes? It would certainly seem that any migration path necessarily requires data changes that rely on the generated data. Thus, initializing the database _requires_ a map from version numbers to data migrations that perform database actions _on the database that exists at that time -- a `Map Version (Aff Unit)` that will act on the database after the given version number is reached from the previous version through a migration. This could just be 'pure unit', but it can't be nothing, and the runtime itself is responsible for using the _correct_ version of the database to do this -- there's no protection from opening the wrong version of the database, except an immediate err because the database doesn't match. And this is the case because each database truly is a _different_ type, with different internal data types representing each table. They might be "nominally" the same, but they're not truly interchangeable because we can't guarantee they would behave the same.
  Thus, any general migration test would intentionally run the migration code from none, to 1, to N, passing through all minor modes and verifying that no errors borne from wrong database changes occurred.
  
As a consequence, we have:

- Schema version definitions of tables, models, table migrations, and so on. Different folders for each.
- Generation of the tables and queries from these definitions, with all versions and targeted versions. This defines all the base items.
- Definition and assignment of data migrations for all versions, from prior versions, OUTSIDE generated files.
- Definition of a database initializer, using the generated one, by passing in the data migration map, OUTSIDE the generated files. Usage of this database initializer to initialize the database in both tests and production. Note that it can err when missing data migrations exist, or are done incorrectly.

It's certainly a lot of files that are generated. But it would appear that this is what's needed to define Serialized Relational Data on the disk, that can nevertheless interact with application code in a type-safe, well-defined way.
  
What is SQL schema version number used for? It's just for our use -- to know what version the runtime has. So it doesn't need to be stored for any SqlLite functionality; it just needs to exist to be tracked generally. A separate table is probably needed here; we just need ONE version for the whole DB, not for individual tables the way the sql_schema table has. The version also helps us know _whether_ a migration is needed -- we examine the version number, do a rough comparison of the existing table schema to verify the actual schema matches compiled schema, and so on.


